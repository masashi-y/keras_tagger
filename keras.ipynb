{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‰ªäÂõû„ÅØ„ÄÅfeedforward neural network„Çí‰Ωø„Å£„Å¶„ÄÅÂõ∫ÂÆö„Ç¶„Ç£„É≥„Éâ„Ç¶ÂπÖ(Â∑¶Âè≥Ôºì)„ÅÆÊÉÖÂ†±„Çí‰Ωø„Å£„Å¶ÂçòË™û„ÅÆÂìÅË©û„Çø„Ç∞„Çí‰∫àÊ∏¨„Åô„Çã„É¢„Éá„É´„ÇíÂ≠¶Áøí„Åó„Å¶„Åø„Åæ„ÅôÔºé  \n",
    "„Åã„Å™„Çä„Ç∑„É≥„Éó„É´„Åß„Åô„Åå„ÄÅÈ´òÈÄü„Å´‰∫àÊ∏¨„Åß„Åç„Çã„Å®„ÅÑ„ÅÜÈ≠ÖÂäõ„Åã„Çâ„ÄÅ„Çà„ÇäÂº∑Âäõ„Å™„É¢„Éá„É´„ÅåÊ≤¢Â±±Â≠òÂú®„Åô„Çã‰ªä„Åß„ÇÇ„ÄÅNLP„ÅÆ„ÅÑ„Çç„Çì„Å™„ÉÑ„Éº„É´„Åß‰Ωø„Çè„Çå„Å¶„Åæ„ÅôÔºé  \n",
    "easyccg: https://github.com/mikelewis0/easyccg  \n",
    "syntaxnet: https://github.com/tensorflow/models/tree/master/research/syntaxnet  \n",
    "stanford parser: https://nlp.stanford.edu/software/lex-parser.shtml  \n",
    "(„Ç§„É°„Éº„Ç∏)„ÄÄ„ÄÄ\n",
    "<img src='images/ff.png'>\n",
    "(ÁîªÂÉè: https://xbt.net/blog/what-is-enigma/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS„Çø„Ç∞(Part-Of-Speech, ÂìÅË©û„Çø„Ç∞)„ÅÆ„É™„Çπ„Éà:  \n",
    "- ADJ: adjective\n",
    "- ADP: adposition\n",
    "- ADV: adverb\n",
    "- AUX: auxiliary\n",
    "- CCONJ: coordinating conjunction\n",
    "- DET: determiner\n",
    "- INTJ: interjection\n",
    "- NOUN: noun\n",
    "- NUM: numeral\n",
    "- PART: particle\n",
    "- PRON: pronoun\n",
    "- PROPN: proper noun\n",
    "- PUNCT: punctuation\n",
    "- SCONJ: subordinating conjunction\n",
    "- SYM: symbol\n",
    "- VERB: verb\n",
    "- X: other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cl/masashi-y/.pyenv_elm13/versions/anaconda3-4.2.0/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# „Å§„Åã„ÅÜ„É©„Ç§„Éñ„É©„É™„ÅÆË™≠„ÅøËæº„Åø\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, Input, Reshape\n",
    "from keras.optimizers import SGD\n",
    "from collections import Counter\n",
    "\n",
    "# üëáÁÑ°Ë¶ñ„Åó„Å¶OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto(\n",
    "    gpu_options=tf.GPUOptions(\n",
    "        visible_device_list=\"0\", # specify GPU number\n",
    "        allow_growth=True\n",
    "    )\n",
    ")\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tNo\tno\tADV\tADV\t_\t7\tdiscourse\twsj_2300.1\t_\r\n",
      "2\t,\t,\tPUNCT\tPUNCT\t_\t7\tpunct\t_\t_\r\n",
      "3\tit\tit\tPRON\tPRON\t_\t7\tnsubj\t_\t_\r\n",
      "4\twas\tbe\tVERB\tVERB\t_\t7\tcop\t_\t_\r\n",
      "5\tn't\tnot\tPART\tPART\t_\t7\tneg\t_\t_\r\n",
      "6\tBlack\tBlack\tPROPN\tPROPN\t_\t7\tcompound\t_\t_\r\n",
      "7\tMonday\tMonday\tPROPN\tPROPN\t_\t0\troot\t_\t_\r\n",
      "8\t.\t.\tPUNCT\tPUNCT\t_\t7\tpunct\t_\t_\r\n",
      "\r\n",
      "1\tOnce\tonce\tADV\tADV\t_\t2\tadvmod\twsj_2300.10\t_\r\n",
      "2\tagain\tagain\tADV\tADV\t_\t9\tadvmod\t_\t_\r\n",
      "3\t-LCB-\t-lcb-\tPUNCT\tPUNCT\t_\t9\tpunct\t_\t_\r\n",
      "4\tthe\tthe\tDET\tDET\t_\t5\tdet\t_\t_\r\n",
      "5\tspecialists\tspecialist\tNOUN\tNOUN\t_\t9\tnsubj\t_\t_\r\n",
      "6\t-RCB-\t-rcb-\tPUNCT\tPUNCT\t_\t9\tpunct\t_\t_\r\n",
      "7\twere\tbe\tVERB\tVERB\t_\t9\tcop\t_\t_\r\n",
      "8\tnot\tnot\tPART\tPART\t_\t9\tneg\t_\t_\r\n",
      "9\table\table\tADJ\tADJ\t_\t24\tccomp\t_\t_\r\n",
      "10\tto\tto\tPART\tPART\t_\t11\tmark\t_\t_\r\n",
      "11\thandle\thandle\tVERB\tVERB\t_\t9\txcomp\t_\t_\r\n"
     ]
    }
   ],
   "source": [
    "# NLP„Åß„Çà„ÅèÂá∫„Å¶„Åè„ÇãCoNLL„Éï„Ç©„Éº„Éû„ÉÉ„Éà\n",
    "!head -20 data/test.conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoNLL„Éï„Ç©„Éº„Éû„ÉÉ„Éà„ÇíË™≠„ÅøËæº„ÇÄÈñ¢Êï∞\n",
    "def read_conll(file):\n",
    "    res = []\n",
    "    words = []\n",
    "    tags = []\n",
    "    for line in open(file):\n",
    "        line = line.strip()\n",
    "         # Á©∫Ë°å„Å™„Çâ„Åù„Çå„Åæ„Åß„Å´„Å§„Åè„Å£„ÅüÊñá„ÇíÂá∫Âäõ\n",
    "        if len(line) == 0:\n",
    "            res.append((words, tags))\n",
    "            words = []\n",
    "            tags = []\n",
    "        # ÂçòË™û„Å®„Çø„Ç∞„ÇíÂèñ„ÇäÂá∫„Åô\n",
    "        else:\n",
    "            items = line.split('\\t')\n",
    "            words.append(items[1].lower()) # Â∞èÊñáÂ≠ó„Å´„Åó„Å¶„Åä„Åè\n",
    "            tags.append(items[3])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Â≠¶Áøí„Éá„Éº„Çø (training data)\n",
    "train_sents = read_conll('data/train.conll')\n",
    "# Ë©ï‰æ°Áî®„Éá„Éº„Çø (test data)\n",
    "test_sents = read_conll('data/test.conll')\n",
    "# ÈñãÁô∫„Éá„Éº„Çø (development data)\n",
    "dev_sents = read_conll('data/dev.conll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no', ',', 'it', 'was', \"n't\", 'black', 'monday', '.']\n",
      "['ADV', 'PUNCT', 'PRON', 'VERB', 'PART', 'PROPN', 'PROPN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "words, tags = test_sents[0] # ‰∏ÄÁï™ÊúÄÂàù„ÅÆÊñá\n",
    "print(words)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_windows(lst):\n",
    "    res = []\n",
    "    for i in range(len(lst) - 6):\n",
    "        res.append(lst[i:i+7])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4, 5, 6],\n",
       " [1, 2, 3, 4, 5, 6, 7],\n",
       " [2, 3, 4, 5, 6, 7, 8],\n",
       " [3, 4, 5, 6, 7, 8, 9]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sliding_windows(list(range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no', ',', 'it', 'was', \"n't\", 'black', 'monday', '.']\n",
      "[['no', ',', 'it', 'was', \"n't\", 'black', 'monday'], [',', 'it', 'was', \"n't\", 'black', 'monday', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(words)\n",
    "print(sliding_windows(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÜ„Åì„Çå„ÅØ„Å†„ÇÅ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = 'PAD'\n",
    "\n",
    "train_sents = [([PAD] * 3 + words + [PAD] * 3, tags) for words, tags in train_sents]\n",
    "test_sents = [([PAD] * 3 + words + [PAD] * 3, tags) for words, tags in test_sents]\n",
    "dev_sents = [([PAD] * 3 + words + [PAD] * 3, tags) for words, tags in dev_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PAD', 'PAD', 'PAD', 'no', ',', 'it', 'was', \"n't\", 'black', 'monday', '.', 'PAD', 'PAD', 'PAD']\n",
      "[['PAD', 'PAD', 'PAD', 'no', ',', 'it', 'was'], ['PAD', 'PAD', 'no', ',', 'it', 'was', \"n't\"], ['PAD', 'no', ',', 'it', 'was', \"n't\", 'black'], ['no', ',', 'it', 'was', \"n't\", 'black', 'monday'], [',', 'it', 'was', \"n't\", 'black', 'monday', '.'], ['it', 'was', \"n't\", 'black', 'monday', '.', 'PAD'], ['was', \"n't\", 'black', 'monday', '.', 'PAD', 'PAD'], [\"n't\", 'black', 'monday', '.', 'PAD', 'PAD', 'PAD']]\n"
     ]
    }
   ],
   "source": [
    "words, tags = test_sents[0]\n",
    "print(words)\n",
    "print(sliding_windows(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ÂçòË™û„ÇíËá™ÁÑ∂Êï∞„ÅÆID„Å´Â§âÊèõ„Åô„ÇãËæûÊõ∏\n",
    "UNK = 'UNK'\n",
    "\n",
    "# ÂçòË™û„ÅÆÂüã„ÇÅËæº„Åø„Éô„ÇØ„Éà„É´„Åå„ÅÜ„Åæ„Åè„ÅÑ„Åè„Åü„ÇÅ„Å´„ÅØ„ÄÅ„Åù„ÅÆÂçòË™û„Åå„ÅÑ„Çç„ÅÑ„Çç„Å™ÊñáËÑà„ÅßÂá∫Áèæ„Åó„Å¶„Åª„Åó„ÅÑÔºé\n",
    "# Â≠¶Áøí„Éá„Éº„Çø„Å´„Å°„Çá„Å£„Å®(ÔºíÂõû„Çà„Çä‰∏ã)„Åó„ÅãÂá∫„Å™„ÅÑÂçòË™û„ÅØUNK„ÅßÁΩÆ„ÅçÊèõ„Åà„ÇãÔºé\n",
    "word_count = Counter(word for words, _ in train_sents for word in words)\n",
    "word_set = [word for word, count in word_count.most_common() if count >= 2]\n",
    "word_set.append(UNK)\n",
    "word_dict = {w: i for i, w in enumerate(word_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS„Çø„Ç∞„ÇíËá™ÁÑ∂Êï∞„ÅÆID„Å´Â§âÊèõ„Åô„ÇãËæûÊõ∏\n",
    "tag_set = set(tag for _, tags in train_sents for tag in tags)\n",
    "tag_dict = {w: i for i, w in enumerate(tag_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_dict size 21569\n",
      "tag_dict size 17\n"
     ]
    }
   ],
   "source": [
    "print('word_dict size', len(word_dict))\n",
    "print('tag_dict size', len(tag_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640\n"
     ]
    }
   ],
   "source": [
    "print(word_dict['dog']) # dog„ÅÆid„ÅØÔºü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# „Éá„Éº„Çø„ÇíË°åÂàó„Å´Â§âÊèõ\n",
    "xs = []\n",
    "ys = []\n",
    "for words, tags in train_sents:\n",
    "    for window in sliding_windows(words):\n",
    "        xs.append([word_dict.get(word, word_dict[UNK]) for word in window])\n",
    "    ys.extend(tag_dict[tag] for tag in tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy„ÅÆË°åÂàó„Å´Â§âÊèõ\n",
    "xs = np.array(xs, 'i')\n",
    "ys = np.array(ys, 'i')\n",
    "ys = keras.utils.to_categorical(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions of xs (929552, 7)\n",
      "dimensions of ys (929552, 17)\n"
     ]
    }
   ],
   "source": [
    "print('dimensions of xs', xs.shape)\n",
    "print('dimensions of ys', ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test, dev„ÇÇË°åÂàó„Å´„Åó„Åü„ÅÑ„ÅÆ„ÅßÈñ¢Êï∞„Å´„Åô„Çã\n",
    "def make_matrices(words_and_tags):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for words, tags in words_and_tags:\n",
    "        for window in sliding_windows(words):\n",
    "            xs.append([word_dict.get(word, word_dict[UNK]) for word in window])\n",
    "        ys.extend(tag_dict[tag] for tag in tags)\n",
    "\n",
    "    xs = np.array(xs, 'i')\n",
    "    ys = np.array(ys, 'i')\n",
    "    ys = keras.utils.to_categorical(ys, len(tag_dict))\n",
    "    print('dimensions of xs', xs.shape)\n",
    "    print('dimensions of ys', ys.shape)\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions of xs (929552, 7)\n",
      "dimensions of ys (929552, 17)\n",
      "dimensions of xs (55371, 7)\n",
      "dimensions of ys (55371, 17)\n",
      "dimensions of xs (45422, 7)\n",
      "dimensions of ys (45422, 17)\n"
     ]
    }
   ],
   "source": [
    "train_xs, train_ys = make_matrices(train_sents)\n",
    "test_xs, test_ys = make_matrices(test_sents)\n",
    "dev_xs, dev_ys = make_matrices(dev_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word_dict)  # ÂçòË™ûÊï∞\n",
    "EMBED_DIM = 128                      # Âüã„ÇÅËæº„Åø„Éô„ÇØ„Éà„É´„ÅÆÊ¨°ÂÖÉÊï∞\n",
    "HIDDEN1_DIM = 256                   # Èö†„ÇåÂ±§Ôºë\n",
    "HIDDEN2_DIM = 128                   # Èö†„ÇåÂ±§Ôºí\n",
    "NUM_TAGS = len(tag_dict)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB_SIZE, EMBED_DIM))\n",
    "model.add(Reshape((EMBED_DIM * 7,)))\n",
    "model.add(Dense(HIDDEN1_DIM, activation='tanh'))\n",
    "model.add(Dense(HIDDEN2_DIM, activation='tanh'))\n",
    "model.add(Dense(NUM_TAGS, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, 'images/model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÂçòË™û„ÅÆIDÂàó${\\bf x} = x_{-2}, x_{-1}, x, x_{+1}, x_{+2}$„Å´ÂØæ„Åó„Å¶  \n",
    "$Embedding(\\bf x) = [ {\\bf e}_{x_{-2}}„ÄÄ| {\\bf e}_{x_{-1}}„ÄÄ| {\\bf e}_{x}„ÄÄ| {\\bf e}_{x_{+1}}„ÄÄ|„ÄÄ{\\bf e}_{x_{+2}} ]^T„ÄÄ= {\\bf E}^T$,  \n",
    "$ Reshape({\\bf E}) = [ {\\bf e}_{x_{-2}}, {\\bf e}_{x_{-1}}, {\\bf e}_{x}, {\\bf e}_{x_{+1}}, {\\bf e}_{x_{+2}} ]^T = {\\bf e}$ (Á∏¶„Å´‰∏¶„Åπ„Çã),  \n",
    "$f({\\bf x}) = {\\mathit softmax}(W_3 \\tanh (W_2 \\tanh (W_1 {\\bf e} + b_1) + b_2) + b_3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ë®àÁÆó„Ç∞„É©„Éï„ÅÆÂèØË¶ñÂåñ\n",
    "<img src='images/model.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         2760832   \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               229632    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 17)                2193      \n",
      "=================================================================\n",
      "Total params: 3,025,553\n",
      "Trainable params: 3,025,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 4787645517783878182, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 7082170778\n",
       " locality {\n",
       "   bus_id: 1\n",
       " }\n",
       " incarnation: 18102384936197633674\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2\", name: \"/device:GPU:1\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 6375404340\n",
       " locality {\n",
       "   bus_id: 1\n",
       " }\n",
       " incarnation: 9913575533825859606\n",
       " physical_device_desc: \"device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0, compute capability: 5.2\"]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 929552 samples, validate on 45422 samples\n",
      "Epoch 1/200\n",
      "929552/929552 [==============================] - 12s 13us/step - loss: 2.4425 - acc: 0.2286 - val_loss: 2.1993 - val_acc: 0.3461\n",
      "Epoch 2/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 1.9336 - acc: 0.4257 - val_loss: 1.7043 - val_acc: 0.4768\n",
      "Epoch 3/200\n",
      "929552/929552 [==============================] - 6s 6us/step - loss: 1.6029 - acc: 0.4992 - val_loss: 1.4710 - val_acc: 0.5298\n",
      "Epoch 4/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 1.3961 - acc: 0.5549 - val_loss: 1.2861 - val_acc: 0.5893\n",
      "Epoch 5/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 1.2266 - acc: 0.6119 - val_loss: 1.1398 - val_acc: 0.6354\n",
      "Epoch 6/200\n",
      "929552/929552 [==============================] - 6s 7us/step - loss: 1.0822 - acc: 0.6569 - val_loss: 1.0063 - val_acc: 0.6788\n",
      "Epoch 7/200\n",
      "929552/929552 [==============================] - 6s 6us/step - loss: 0.9473 - acc: 0.6995 - val_loss: 0.8844 - val_acc: 0.7173\n",
      "Epoch 8/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.8316 - acc: 0.7342 - val_loss: 0.7854 - val_acc: 0.7451\n",
      "Epoch 9/200\n",
      "929552/929552 [==============================] - 6s 7us/step - loss: 0.7414 - acc: 0.7583 - val_loss: 0.7089 - val_acc: 0.7677\n",
      "Epoch 10/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.6708 - acc: 0.7789 - val_loss: 0.6476 - val_acc: 0.7867\n",
      "Epoch 11/200\n",
      "929552/929552 [==============================] - 7s 8us/step - loss: 0.6125 - acc: 0.7978 - val_loss: 0.5956 - val_acc: 0.8055\n",
      "Epoch 12/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.5627 - acc: 0.8147 - val_loss: 0.5516 - val_acc: 0.8202\n",
      "Epoch 13/200\n",
      "929552/929552 [==============================] - 6s 6us/step - loss: 0.5196 - acc: 0.8297 - val_loss: 0.5133 - val_acc: 0.8333\n",
      "Epoch 14/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.4822 - acc: 0.8424 - val_loss: 0.4803 - val_acc: 0.8447\n",
      "Epoch 15/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.4498 - acc: 0.8537 - val_loss: 0.4524 - val_acc: 0.8538\n",
      "Epoch 16/200\n",
      "929552/929552 [==============================] - 6s 6us/step - loss: 0.4218 - acc: 0.8629 - val_loss: 0.4275 - val_acc: 0.8629\n",
      "Epoch 17/200\n",
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.3975 - acc: 0.8711 - val_loss: 0.4066 - val_acc: 0.8697\n",
      "Epoch 18/200\n",
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.3764 - acc: 0.8781 - val_loss: 0.3883 - val_acc: 0.8759\n",
      "Epoch 19/200\n",
      "929552/929552 [==============================] - 4s 5us/step - loss: 0.3579 - acc: 0.8840 - val_loss: 0.3719 - val_acc: 0.8805\n",
      "Epoch 20/200\n",
      "929552/929552 [==============================] - 7s 7us/step - loss: 0.3417 - acc: 0.8891 - val_loss: 0.3581 - val_acc: 0.8848\n",
      "Epoch 21/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.3273 - acc: 0.8938 - val_loss: 0.3452 - val_acc: 0.8892\n",
      "Epoch 22/200\n",
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.3144 - acc: 0.8979 - val_loss: 0.3343 - val_acc: 0.8930\n",
      "Epoch 23/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.3029 - acc: 0.9017 - val_loss: 0.3245 - val_acc: 0.8957\n",
      "Epoch 24/200\n",
      "929552/929552 [==============================] - 7s 7us/step - loss: 0.2925 - acc: 0.9049 - val_loss: 0.3154 - val_acc: 0.8987\n",
      "Epoch 25/200\n",
      "929552/929552 [==============================] - 3s 4us/step - loss: 0.2830 - acc: 0.9079 - val_loss: 0.3076 - val_acc: 0.9009\n",
      "Epoch 26/200\n",
      "929552/929552 [==============================] - 6s 6us/step - loss: 0.2744 - acc: 0.9107 - val_loss: 0.2995 - val_acc: 0.9038\n",
      "Epoch 27/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.2664 - acc: 0.9134 - val_loss: 0.2931 - val_acc: 0.9055\n",
      "Epoch 28/200\n",
      "929552/929552 [==============================] - 7s 8us/step - loss: 0.2591 - acc: 0.9157 - val_loss: 0.2869 - val_acc: 0.9072\n",
      "Epoch 29/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.2523 - acc: 0.9179 - val_loss: 0.2813 - val_acc: 0.9091\n",
      "Epoch 30/200\n",
      "929552/929552 [==============================] - 6s 6us/step - loss: 0.2460 - acc: 0.9199 - val_loss: 0.2757 - val_acc: 0.9104\n",
      "Epoch 31/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.2401 - acc: 0.9219 - val_loss: 0.2709 - val_acc: 0.9125\n",
      "Epoch 32/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.2346 - acc: 0.9236 - val_loss: 0.2666 - val_acc: 0.9135\n",
      "Epoch 33/200\n",
      "929552/929552 [==============================] - 8s 8us/step - loss: 0.2294 - acc: 0.9254 - val_loss: 0.2622 - val_acc: 0.9151\n",
      "Epoch 34/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.2246 - acc: 0.9269 - val_loss: 0.2585 - val_acc: 0.9159\n",
      "Epoch 35/200\n",
      "929552/929552 [==============================] - 6s 6us/step - loss: 0.2200 - acc: 0.9285 - val_loss: 0.2548 - val_acc: 0.9172\n",
      "Epoch 36/200\n",
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.2157 - acc: 0.9298 - val_loss: 0.2514 - val_acc: 0.9182\n",
      "Epoch 37/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.2116 - acc: 0.9312 - val_loss: 0.2477 - val_acc: 0.9193\n",
      "Epoch 38/200\n",
      "929552/929552 [==============================] - 6s 6us/step - loss: 0.2077 - acc: 0.9324 - val_loss: 0.2445 - val_acc: 0.9210\n",
      "Epoch 39/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.2040 - acc: 0.9336 - val_loss: 0.2424 - val_acc: 0.9212\n",
      "Epoch 40/200\n",
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.2005 - acc: 0.9348 - val_loss: 0.2392 - val_acc: 0.9230\n",
      "Epoch 41/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.1971 - acc: 0.9359 - val_loss: 0.2369 - val_acc: 0.9236\n",
      "Epoch 42/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.1939 - acc: 0.9370 - val_loss: 0.2340 - val_acc: 0.9243\n",
      "Epoch 43/200\n",
      "929552/929552 [==============================] - 8s 9us/step - loss: 0.1908 - acc: 0.9380 - val_loss: 0.2318 - val_acc: 0.9254\n",
      "Epoch 44/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.1879 - acc: 0.9390 - val_loss: 0.2293 - val_acc: 0.9257\n",
      "Epoch 45/200\n",
      "929552/929552 [==============================] - 6s 6us/step - loss: 0.1851 - acc: 0.9400 - val_loss: 0.2278 - val_acc: 0.9270\n",
      "Epoch 46/200\n",
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.1824 - acc: 0.9409 - val_loss: 0.2255 - val_acc: 0.9274\n",
      "Epoch 47/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.1798 - acc: 0.9417 - val_loss: 0.2232 - val_acc: 0.9286\n",
      "Epoch 48/200\n",
      "929552/929552 [==============================] - 8s 8us/step - loss: 0.1773 - acc: 0.9425 - val_loss: 0.2217 - val_acc: 0.9287\n",
      "Epoch 49/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.1749 - acc: 0.9433 - val_loss: 0.2196 - val_acc: 0.9293\n",
      "Epoch 50/200\n",
      "929552/929552 [==============================] - 6s 6us/step - loss: 0.1726 - acc: 0.9440 - val_loss: 0.2179 - val_acc: 0.9293\n",
      "Epoch 51/200\n",
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.1704 - acc: 0.9447 - val_loss: 0.2166 - val_acc: 0.9304\n",
      "Epoch 52/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.1683 - acc: 0.9453 - val_loss: 0.2155 - val_acc: 0.9302\n",
      "Epoch 53/200\n",
      "929552/929552 [==============================] - 8s 8us/step - loss: 0.1662 - acc: 0.9461 - val_loss: 0.2145 - val_acc: 0.9307\n",
      "Epoch 54/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.1642 - acc: 0.9468 - val_loss: 0.2121 - val_acc: 0.9316\n",
      "Epoch 55/200\n",
      "929552/929552 [==============================] - 6s 6us/step - loss: 0.1623 - acc: 0.9474 - val_loss: 0.2110 - val_acc: 0.9321\n",
      "Epoch 56/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.1604 - acc: 0.9480 - val_loss: 0.2103 - val_acc: 0.9322\n",
      "Epoch 57/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.1586 - acc: 0.9485 - val_loss: 0.2085 - val_acc: 0.9332\n",
      "Epoch 58/200\n",
      "929552/929552 [==============================] - 7s 7us/step - loss: 0.1569 - acc: 0.9492 - val_loss: 0.2079 - val_acc: 0.9335\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.1552 - acc: 0.9498 - val_loss: 0.2062 - val_acc: 0.9339\n",
      "Epoch 60/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.1536 - acc: 0.9502 - val_loss: 0.2053 - val_acc: 0.9343\n",
      "Epoch 61/200\n",
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.1520 - acc: 0.9508 - val_loss: 0.2043 - val_acc: 0.9348\n",
      "Epoch 62/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.1505 - acc: 0.9513 - val_loss: 0.2031 - val_acc: 0.9352\n",
      "Epoch 63/200\n",
      "929552/929552 [==============================] - 4s 5us/step - loss: 0.1490 - acc: 0.9518 - val_loss: 0.2017 - val_acc: 0.9355\n",
      "Epoch 64/200\n",
      "929552/929552 [==============================] - 7s 7us/step - loss: 0.1475 - acc: 0.9523 - val_loss: 0.2009 - val_acc: 0.9359\n",
      "Epoch 65/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.1461 - acc: 0.9528 - val_loss: 0.2004 - val_acc: 0.9362\n",
      "Epoch 66/200\n",
      "929552/929552 [==============================] - 6s 7us/step - loss: 0.1448 - acc: 0.9532 - val_loss: 0.1987 - val_acc: 0.9367\n",
      "Epoch 67/200\n",
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.1434 - acc: 0.9537 - val_loss: 0.1985 - val_acc: 0.9364\n",
      "Epoch 68/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.1421 - acc: 0.9541 - val_loss: 0.1972 - val_acc: 0.9372\n",
      "Epoch 69/200\n",
      "929552/929552 [==============================] - 8s 9us/step - loss: 0.1409 - acc: 0.9545 - val_loss: 0.1980 - val_acc: 0.9366\n",
      "Epoch 70/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.1397 - acc: 0.9549 - val_loss: 0.1961 - val_acc: 0.9373\n",
      "Epoch 71/200\n",
      "929552/929552 [==============================] - 6s 6us/step - loss: 0.1385 - acc: 0.9553 - val_loss: 0.1952 - val_acc: 0.9381\n",
      "Epoch 72/200\n",
      "929552/929552 [==============================] - 4s 5us/step - loss: 0.1373 - acc: 0.9557 - val_loss: 0.1952 - val_acc: 0.9380\n",
      "Epoch 73/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.1362 - acc: 0.9561 - val_loss: 0.1941 - val_acc: 0.9381\n",
      "Epoch 74/200\n",
      "929552/929552 [==============================] - 7s 8us/step - loss: 0.1351 - acc: 0.9564 - val_loss: 0.1937 - val_acc: 0.9377\n",
      "Epoch 75/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.1340 - acc: 0.9567 - val_loss: 0.1933 - val_acc: 0.9385\n",
      "Epoch 76/200\n",
      "929552/929552 [==============================] - 6s 6us/step - loss: 0.1330 - acc: 0.9570 - val_loss: 0.1925 - val_acc: 0.9389\n",
      "Epoch 77/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.1319 - acc: 0.9574 - val_loss: 0.1914 - val_acc: 0.9393\n",
      "Epoch 78/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.1310 - acc: 0.9577 - val_loss: 0.1916 - val_acc: 0.9390\n",
      "Epoch 79/200\n",
      "929552/929552 [==============================] - 7s 8us/step - loss: 0.1300 - acc: 0.9580 - val_loss: 0.1902 - val_acc: 0.9396\n",
      "Epoch 80/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.1291 - acc: 0.9583 - val_loss: 0.1905 - val_acc: 0.9392\n",
      "Epoch 81/200\n",
      "929552/929552 [==============================] - 6s 6us/step - loss: 0.1281 - acc: 0.9586 - val_loss: 0.1898 - val_acc: 0.9399\n",
      "Epoch 82/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.1272 - acc: 0.9589 - val_loss: 0.1892 - val_acc: 0.9399\n",
      "Epoch 83/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.1263 - acc: 0.9592 - val_loss: 0.1890 - val_acc: 0.9400\n",
      "Epoch 84/200\n",
      "929552/929552 [==============================] - 6s 6us/step - loss: 0.1255 - acc: 0.9594 - val_loss: 0.1878 - val_acc: 0.9402\n",
      "Epoch 85/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.1246 - acc: 0.9597 - val_loss: 0.1881 - val_acc: 0.9405\n",
      "Epoch 86/200\n",
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.1238 - acc: 0.9600 - val_loss: 0.1877 - val_acc: 0.9410\n",
      "Epoch 87/200\n",
      "929552/929552 [==============================] - 6s 6us/step - loss: 0.1230 - acc: 0.9602 - val_loss: 0.1876 - val_acc: 0.9405\n",
      "Epoch 88/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.1222 - acc: 0.9605 - val_loss: 0.1863 - val_acc: 0.9414\n",
      "Epoch 89/200\n",
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.1214 - acc: 0.9608 - val_loss: 0.1861 - val_acc: 0.9412\n",
      "Epoch 90/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.1206 - acc: 0.9609 - val_loss: 0.1854 - val_acc: 0.9417\n",
      "Epoch 91/200\n",
      "929552/929552 [==============================] - 7s 7us/step - loss: 0.1199 - acc: 0.9612 - val_loss: 0.1869 - val_acc: 0.9414\n",
      "Epoch 92/200\n",
      "929552/929552 [==============================] - 3s 4us/step - loss: 0.1192 - acc: 0.9614 - val_loss: 0.1853 - val_acc: 0.9417\n",
      "Epoch 93/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.1185 - acc: 0.9617 - val_loss: 0.1850 - val_acc: 0.9415\n",
      "Epoch 94/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.1178 - acc: 0.9619 - val_loss: 0.1853 - val_acc: 0.9418\n",
      "Epoch 95/200\n",
      "929552/929552 [==============================] - 4s 5us/step - loss: 0.1171 - acc: 0.9622 - val_loss: 0.1850 - val_acc: 0.9421\n",
      "Epoch 96/200\n",
      "929552/929552 [==============================] - 8s 8us/step - loss: 0.1164 - acc: 0.9624 - val_loss: 0.1839 - val_acc: 0.9425\n",
      "Epoch 97/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.1157 - acc: 0.9625 - val_loss: 0.1849 - val_acc: 0.9419\n",
      "Epoch 98/200\n",
      "929552/929552 [==============================] - 6s 6us/step - loss: 0.1151 - acc: 0.9628 - val_loss: 0.1844 - val_acc: 0.9419\n",
      "Epoch 99/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.1145 - acc: 0.9630 - val_loss: 0.1830 - val_acc: 0.9425\n",
      "Epoch 100/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.1138 - acc: 0.9631 - val_loss: 0.1836 - val_acc: 0.9422\n",
      "Epoch 101/200\n",
      "929552/929552 [==============================] - 7s 7us/step - loss: 0.1132 - acc: 0.9634 - val_loss: 0.1836 - val_acc: 0.9426\n",
      "Epoch 102/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.1126 - acc: 0.9635 - val_loss: 0.1829 - val_acc: 0.9427\n",
      "Epoch 103/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.1120 - acc: 0.9637 - val_loss: 0.1829 - val_acc: 0.9428\n",
      "Epoch 104/200\n",
      "929552/929552 [==============================] - 6s 6us/step - loss: 0.1114 - acc: 0.9639 - val_loss: 0.1829 - val_acc: 0.9428\n",
      "Epoch 105/200\n",
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.1109 - acc: 0.9641 - val_loss: 0.1828 - val_acc: 0.9429\n",
      "Epoch 106/200\n",
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.1103 - acc: 0.9643 - val_loss: 0.1828 - val_acc: 0.9432\n",
      "Epoch 107/200\n",
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.1098 - acc: 0.9644 - val_loss: 0.1825 - val_acc: 0.9433\n",
      "Epoch 108/200\n",
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.1092 - acc: 0.9646 - val_loss: 0.1814 - val_acc: 0.9437\n",
      "Epoch 109/200\n",
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.1087 - acc: 0.9647 - val_loss: 0.1822 - val_acc: 0.9436\n",
      "Epoch 110/200\n",
      "929552/929552 [==============================] - 4s 5us/step - loss: 0.1081 - acc: 0.9650 - val_loss: 0.1814 - val_acc: 0.9437\n",
      "Epoch 111/200\n",
      "929552/929552 [==============================] - 8s 8us/step - loss: 0.1076 - acc: 0.9651 - val_loss: 0.1813 - val_acc: 0.9436\n",
      "Epoch 112/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.1071 - acc: 0.9652 - val_loss: 0.1810 - val_acc: 0.9443\n",
      "Epoch 113/200\n",
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.1066 - acc: 0.9654 - val_loss: 0.1813 - val_acc: 0.9442\n",
      "Epoch 114/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.1061 - acc: 0.9655 - val_loss: 0.1807 - val_acc: 0.9438\n",
      "Epoch 115/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.1056 - acc: 0.9657 - val_loss: 0.1804 - val_acc: 0.9445\n",
      "Epoch 116/200\n",
      "929552/929552 [==============================] - 6s 6us/step - loss: 0.1051 - acc: 0.9659 - val_loss: 0.1819 - val_acc: 0.9439\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.1047 - acc: 0.9661 - val_loss: 0.1812 - val_acc: 0.9436\n",
      "Epoch 118/200\n",
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.1042 - acc: 0.9662 - val_loss: 0.1804 - val_acc: 0.9444\n",
      "Epoch 119/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.1037 - acc: 0.9662 - val_loss: 0.1809 - val_acc: 0.9444\n",
      "Epoch 120/200\n",
      "929552/929552 [==============================] - 7s 8us/step - loss: 0.1033 - acc: 0.9665 - val_loss: 0.1808 - val_acc: 0.9444\n",
      "Epoch 121/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.1028 - acc: 0.9666 - val_loss: 0.1807 - val_acc: 0.9443\n",
      "Epoch 122/200\n",
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.1024 - acc: 0.9668 - val_loss: 0.1805 - val_acc: 0.9442\n",
      "Epoch 123/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.1019 - acc: 0.9668 - val_loss: 0.1800 - val_acc: 0.9443\n",
      "Epoch 124/200\n",
      "929552/929552 [==============================] - 7s 7us/step - loss: 0.1015 - acc: 0.9670 - val_loss: 0.1799 - val_acc: 0.9443\n",
      "Epoch 125/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.1011 - acc: 0.9670 - val_loss: 0.1803 - val_acc: 0.9445\n",
      "Epoch 126/200\n",
      "929552/929552 [==============================] - 6s 6us/step - loss: 0.1006 - acc: 0.9672 - val_loss: 0.1798 - val_acc: 0.9444\n",
      "Epoch 127/200\n",
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.1002 - acc: 0.9673 - val_loss: 0.1793 - val_acc: 0.9449\n",
      "Epoch 128/200\n",
      "929552/929552 [==============================] - 4s 5us/step - loss: 0.0998 - acc: 0.9674 - val_loss: 0.1796 - val_acc: 0.9447\n",
      "Epoch 129/200\n",
      "929552/929552 [==============================] - 7s 8us/step - loss: 0.0994 - acc: 0.9676 - val_loss: 0.1800 - val_acc: 0.9447\n",
      "Epoch 130/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.0990 - acc: 0.9678 - val_loss: 0.1794 - val_acc: 0.9448\n",
      "Epoch 131/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.0986 - acc: 0.9679 - val_loss: 0.1791 - val_acc: 0.9452\n",
      "Epoch 132/200\n",
      "929552/929552 [==============================] - 6s 6us/step - loss: 0.0982 - acc: 0.9679 - val_loss: 0.1803 - val_acc: 0.9447\n",
      "Epoch 133/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.0978 - acc: 0.9681 - val_loss: 0.1789 - val_acc: 0.9454\n",
      "Epoch 134/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.0974 - acc: 0.9682 - val_loss: 0.1795 - val_acc: 0.9453\n",
      "Epoch 135/200\n",
      "929552/929552 [==============================] - 8s 8us/step - loss: 0.0970 - acc: 0.9684 - val_loss: 0.1788 - val_acc: 0.9452\n",
      "Epoch 136/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.0966 - acc: 0.9685 - val_loss: 0.1793 - val_acc: 0.9449\n",
      "Epoch 137/200\n",
      "929552/929552 [==============================] - 6s 7us/step - loss: 0.0963 - acc: 0.9686 - val_loss: 0.1801 - val_acc: 0.9449\n",
      "Epoch 138/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.0959 - acc: 0.9687 - val_loss: 0.1798 - val_acc: 0.9450\n",
      "Epoch 139/200\n",
      "929552/929552 [==============================] - 6s 7us/step - loss: 0.0955 - acc: 0.9688 - val_loss: 0.1802 - val_acc: 0.9454\n",
      "Epoch 140/200\n",
      "929552/929552 [==============================] - 4s 5us/step - loss: 0.0951 - acc: 0.9688 - val_loss: 0.1794 - val_acc: 0.9455\n",
      "Epoch 141/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.0948 - acc: 0.9690 - val_loss: 0.1792 - val_acc: 0.9455\n",
      "Epoch 142/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.0944 - acc: 0.9691 - val_loss: 0.1796 - val_acc: 0.9453\n",
      "Epoch 143/200\n",
      "929552/929552 [==============================] - 4s 5us/step - loss: 0.0940 - acc: 0.9692 - val_loss: 0.1804 - val_acc: 0.9452\n",
      "Epoch 144/200\n",
      "929552/929552 [==============================] - 7s 7us/step - loss: 0.0937 - acc: 0.9693 - val_loss: 0.1801 - val_acc: 0.9456\n",
      "Epoch 145/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.0934 - acc: 0.9694 - val_loss: 0.1808 - val_acc: 0.9451\n",
      "Epoch 146/200\n",
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.0930 - acc: 0.9695 - val_loss: 0.1798 - val_acc: 0.9457\n",
      "Epoch 147/200\n",
      "929552/929552 [==============================] - 4s 5us/step - loss: 0.0927 - acc: 0.9696 - val_loss: 0.1803 - val_acc: 0.9453\n",
      "Epoch 148/200\n",
      "929552/929552 [==============================] - 7s 8us/step - loss: 0.0923 - acc: 0.9697 - val_loss: 0.1795 - val_acc: 0.9460\n",
      "Epoch 149/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.0920 - acc: 0.9698 - val_loss: 0.1796 - val_acc: 0.9452\n",
      "Epoch 150/200\n",
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.0916 - acc: 0.9699 - val_loss: 0.1794 - val_acc: 0.9454\n",
      "Epoch 151/200\n",
      "929552/929552 [==============================] - 4s 5us/step - loss: 0.0913 - acc: 0.9700 - val_loss: 0.1807 - val_acc: 0.9450\n",
      "Epoch 152/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.0910 - acc: 0.9701 - val_loss: 0.1797 - val_acc: 0.9450\n",
      "Epoch 153/200\n",
      "929552/929552 [==============================] - 6s 6us/step - loss: 0.0906 - acc: 0.9703 - val_loss: 0.1800 - val_acc: 0.9455\n",
      "Epoch 154/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.0903 - acc: 0.9704 - val_loss: 0.1792 - val_acc: 0.9458\n",
      "Epoch 155/200\n",
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.0900 - acc: 0.9705 - val_loss: 0.1794 - val_acc: 0.9458\n",
      "Epoch 156/200\n",
      "929552/929552 [==============================] - 4s 5us/step - loss: 0.0897 - acc: 0.9706 - val_loss: 0.1811 - val_acc: 0.9458\n",
      "Epoch 157/200\n",
      "929552/929552 [==============================] - 7s 8us/step - loss: 0.0894 - acc: 0.9706 - val_loss: 0.1803 - val_acc: 0.9453\n",
      "Epoch 158/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.0890 - acc: 0.9708 - val_loss: 0.1796 - val_acc: 0.9456\n",
      "Epoch 159/200\n",
      "929552/929552 [==============================] - 6s 6us/step - loss: 0.0887 - acc: 0.9710 - val_loss: 0.1797 - val_acc: 0.9458\n",
      "Epoch 160/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.0884 - acc: 0.9709 - val_loss: 0.1797 - val_acc: 0.9453\n",
      "Epoch 161/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.0881 - acc: 0.9710 - val_loss: 0.1809 - val_acc: 0.9452\n",
      "Epoch 162/200\n",
      "929552/929552 [==============================] - 7s 7us/step - loss: 0.0878 - acc: 0.9713 - val_loss: 0.1801 - val_acc: 0.9458\n",
      "Epoch 163/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.0875 - acc: 0.9713 - val_loss: 0.1808 - val_acc: 0.9452\n",
      "Epoch 164/200\n",
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.0872 - acc: 0.9714 - val_loss: 0.1801 - val_acc: 0.9454\n",
      "Epoch 165/200\n",
      "929552/929552 [==============================] - 6s 6us/step - loss: 0.0869 - acc: 0.9715 - val_loss: 0.1800 - val_acc: 0.9457\n",
      "Epoch 166/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.0866 - acc: 0.9716 - val_loss: 0.1818 - val_acc: 0.9453\n",
      "Epoch 167/200\n",
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.0863 - acc: 0.9717 - val_loss: 0.1808 - val_acc: 0.9452\n",
      "Epoch 168/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.0860 - acc: 0.9719 - val_loss: 0.1811 - val_acc: 0.9458\n",
      "Epoch 169/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.0857 - acc: 0.9719 - val_loss: 0.1804 - val_acc: 0.9459\n",
      "Epoch 170/200\n",
      "929552/929552 [==============================] - 8s 8us/step - loss: 0.0854 - acc: 0.9720 - val_loss: 0.1816 - val_acc: 0.9453\n",
      "Epoch 171/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.0851 - acc: 0.9722 - val_loss: 0.1806 - val_acc: 0.9457\n",
      "Epoch 172/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.0848 - acc: 0.9722 - val_loss: 0.1812 - val_acc: 0.9458\n",
      "Epoch 173/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.0845 - acc: 0.9724 - val_loss: 0.1809 - val_acc: 0.9457\n",
      "Epoch 174/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.0842 - acc: 0.9724 - val_loss: 0.1813 - val_acc: 0.9458\n",
      "Epoch 175/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "929552/929552 [==============================] - 7s 8us/step - loss: 0.0840 - acc: 0.9725 - val_loss: 0.1812 - val_acc: 0.9455\n",
      "Epoch 176/200\n",
      "929552/929552 [==============================] - 3s 4us/step - loss: 0.0837 - acc: 0.9725 - val_loss: 0.1812 - val_acc: 0.9457\n",
      "Epoch 177/200\n",
      "929552/929552 [==============================] - 6s 6us/step - loss: 0.0834 - acc: 0.9727 - val_loss: 0.1815 - val_acc: 0.9459\n",
      "Epoch 178/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.0831 - acc: 0.9727 - val_loss: 0.1818 - val_acc: 0.9456\n",
      "Epoch 179/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.0828 - acc: 0.9729 - val_loss: 0.1810 - val_acc: 0.9456\n",
      "Epoch 180/200\n",
      "929552/929552 [==============================] - 8s 8us/step - loss: 0.0826 - acc: 0.9731 - val_loss: 0.1820 - val_acc: 0.9458\n",
      "Epoch 181/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.0823 - acc: 0.9730 - val_loss: 0.1813 - val_acc: 0.9460\n",
      "Epoch 182/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.0820 - acc: 0.9731 - val_loss: 0.1821 - val_acc: 0.9458\n",
      "Epoch 183/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.0817 - acc: 0.9733 - val_loss: 0.1821 - val_acc: 0.9458\n",
      "Epoch 184/200\n",
      "929552/929552 [==============================] - 4s 5us/step - loss: 0.0815 - acc: 0.9733 - val_loss: 0.1816 - val_acc: 0.9453\n",
      "Epoch 185/200\n",
      "929552/929552 [==============================] - 7s 7us/step - loss: 0.0812 - acc: 0.9734 - val_loss: 0.1822 - val_acc: 0.9458\n",
      "Epoch 186/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.0809 - acc: 0.9736 - val_loss: 0.1826 - val_acc: 0.9461\n",
      "Epoch 187/200\n",
      "929552/929552 [==============================] - 6s 7us/step - loss: 0.0807 - acc: 0.9736 - val_loss: 0.1819 - val_acc: 0.9459\n",
      "Epoch 188/200\n",
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.0804 - acc: 0.9738 - val_loss: 0.1819 - val_acc: 0.9458\n",
      "Epoch 189/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.0801 - acc: 0.9738 - val_loss: 0.1835 - val_acc: 0.9459\n",
      "Epoch 190/200\n",
      "929552/929552 [==============================] - 8s 9us/step - loss: 0.0799 - acc: 0.9739 - val_loss: 0.1824 - val_acc: 0.9458\n",
      "Epoch 191/200\n",
      "929552/929552 [==============================] - 3s 4us/step - loss: 0.0796 - acc: 0.9739 - val_loss: 0.1834 - val_acc: 0.9458\n",
      "Epoch 192/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.0793 - acc: 0.9741 - val_loss: 0.1827 - val_acc: 0.9459\n",
      "Epoch 193/200\n",
      "929552/929552 [==============================] - 6s 6us/step - loss: 0.0791 - acc: 0.9742 - val_loss: 0.1829 - val_acc: 0.9453\n",
      "Epoch 194/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.0788 - acc: 0.9742 - val_loss: 0.1837 - val_acc: 0.9460\n",
      "Epoch 195/200\n",
      "929552/929552 [==============================] - 6s 6us/step - loss: 0.0785 - acc: 0.9743 - val_loss: 0.1838 - val_acc: 0.9453\n",
      "Epoch 196/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.0783 - acc: 0.9744 - val_loss: 0.1839 - val_acc: 0.9458\n",
      "Epoch 197/200\n",
      "929552/929552 [==============================] - 5s 6us/step - loss: 0.0780 - acc: 0.9745 - val_loss: 0.1846 - val_acc: 0.9456\n",
      "Epoch 198/200\n",
      "929552/929552 [==============================] - 5s 5us/step - loss: 0.0778 - acc: 0.9746 - val_loss: 0.1837 - val_acc: 0.9460\n",
      "Epoch 199/200\n",
      "929552/929552 [==============================] - 4s 4us/step - loss: 0.0775 - acc: 0.9748 - val_loss: 0.1848 - val_acc: 0.9451\n",
      "Epoch 200/200\n",
      "929552/929552 [==============================] - 7s 8us/step - loss: 0.0773 - acc: 0.9748 - val_loss: 0.1847 - val_acc: 0.9458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa3834db7f0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Â≠¶Áøí\n",
    "model.fit(train_xs, train_ys, batch_size=1024, epochs=200, verbose=1, validation_data=(dev_xs, dev_ys))\n",
    "\n",
    "# Â≠¶Áøí„Åå„ÇÅ„Çì„Å©„ÅÑÂ†¥Âêà„Åì„Å£„Å° (Â≠¶ÁøíÊ∏à„Åø„ÅÆ„Éë„É©„É°„Éº„Çø„ÇíË™≠„ÅøËæº„Åø)\n",
    "# model.load_weights('models/weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.10557082e-02, 1.32073769e-02, 7.10472639e-04, 7.18352795e-01,\n",
       "        5.89033652e-06, 3.89746914e-04, 5.57578518e-04, 2.58904400e-08,\n",
       "        1.58037595e-03, 2.01247330e-03, 1.02006830e-03, 7.10095046e-04,\n",
       "        4.66211577e-06, 2.16930785e-04, 3.60750452e-09, 1.80175677e-01,\n",
       "        7.04472061e-16],\n",
       "       [7.86494547e-06, 2.18097651e-09, 1.24326913e-08, 6.10316864e-13,\n",
       "        5.12558381e-06, 3.25297265e-07, 2.90912703e-05, 5.02471835e-07,\n",
       "        8.15867799e-16, 5.47013144e-07, 9.99931097e-01, 5.55866075e-10,\n",
       "        3.77816776e-14, 2.23222092e-12, 2.88937202e-14, 2.54073948e-05,\n",
       "        2.39162648e-17],\n",
       "       [5.88764760e-06, 9.99991417e-01, 3.21845732e-08, 2.37876435e-07,\n",
       "        4.31857744e-11, 3.83840426e-09, 3.75526753e-07, 3.62298209e-07,\n",
       "        8.90485342e-12, 1.44225919e-06, 1.40812717e-09, 4.49098270e-09,\n",
       "        4.32071739e-12, 1.78185413e-08, 1.58697949e-16, 3.02612193e-07,\n",
       "        8.71168284e-21],\n",
       "       [2.78762332e-06, 2.99380929e-07, 6.22993355e-08, 1.89591069e-11,\n",
       "        1.18249865e-07, 1.12220647e-07, 6.04438222e-10, 1.85235992e-01,\n",
       "        5.48263171e-11, 1.28865878e-08, 2.74877276e-09, 4.82539910e-08,\n",
       "        6.48731069e-10, 3.09250083e-07, 8.14760208e-01, 4.03657874e-13,\n",
       "        1.05973879e-08],\n",
       "       [1.55373523e-03, 1.84848297e-08, 1.27983483e-07, 3.61202375e-08,\n",
       "        1.99484120e-07, 9.98344898e-01, 1.66643431e-07, 1.11149518e-06,\n",
       "        5.93643961e-08, 9.60529214e-05, 1.16685487e-06, 1.38816958e-07,\n",
       "        2.12380428e-06, 1.55193938e-08, 2.84002266e-09, 4.37437361e-17,\n",
       "        7.41189291e-21],\n",
       "       [3.38509599e-05, 3.43298345e-09, 8.48578452e-07, 5.98736115e-05,\n",
       "        1.94610238e-05, 2.16799965e-08, 4.65684842e-08, 3.34189981e-10,\n",
       "        9.97890413e-01, 1.27522048e-09, 1.06249828e-07, 3.97084732e-05,\n",
       "        6.85932446e-06, 4.27011810e-06, 1.46860583e-03, 1.02734513e-04,\n",
       "        3.73150455e-04],\n",
       "       [2.15060467e-07, 1.30863654e-07, 8.88689371e-08, 1.33376261e-07,\n",
       "        1.53163683e-06, 1.44930859e-10, 2.55603947e-08, 2.93949262e-12,\n",
       "        6.53779466e-07, 3.55013935e-11, 5.47567026e-07, 2.31251832e-07,\n",
       "        2.78216700e-10, 5.89388804e-08, 3.30027700e-10, 9.99996066e-01,\n",
       "        3.37383227e-07],\n",
       "       [5.95342044e-06, 1.21922944e-10, 6.79182888e-09, 3.66117808e-13,\n",
       "        9.38162793e-06, 1.22710344e-06, 4.08636424e-06, 2.96525915e-08,\n",
       "        5.66208618e-15, 4.20304396e-08, 9.99974370e-01, 5.50276935e-10,\n",
       "        8.51077347e-13, 9.67689687e-13, 4.12879718e-13, 4.90602224e-06,\n",
       "        7.24227358e-16],\n",
       "       [9.81510162e-01, 8.19611669e-05, 1.43830066e-05, 8.13563020e-07,\n",
       "        2.96430926e-07, 2.92374374e-04, 9.35229997e-04, 3.83749330e-06,\n",
       "        8.13145675e-07, 1.70841292e-02, 2.90051648e-05, 5.34786341e-06,\n",
       "        4.16305156e-05, 1.45383567e-07, 2.79617052e-10, 7.36621208e-10,\n",
       "        3.66972048e-16],\n",
       "       [9.98727620e-01, 7.58225360e-05, 1.28597254e-04, 6.25387713e-07,\n",
       "        4.86604004e-05, 3.42586668e-06, 1.14824201e-04, 1.25931717e-06,\n",
       "        5.70837059e-04, 8.22627226e-06, 3.24577850e-05, 1.64633122e-04,\n",
       "        2.74407762e-06, 4.46088325e-06, 4.67201280e-05, 6.78745782e-05,\n",
       "        1.22796371e-06]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Â≠¶Áøí„Åó„Åü„É¢„Éá„É´„ÇíÈÅ©ÂΩì„Å´‰Ωø„Å£„Å¶„Åø„Çã„Å®„Å™„Çì„ÅãË°åÂàó„ÅåÂá∫„Å¶„Åç„Åæ„Åô\n",
    "model.predict(test_xs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 17)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# „Åù„ÅÆË°åÂàó„ÅÆÂΩ¢\n",
    "Out[27].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS„Çø„Ç∞„Å®ID„ÅÆÈÄÜÂêë„Åç„ÅÆËæûÊõ∏\n",
    "rev_tag_dict = {v: k for k, v in tag_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DET', 'PUNCT', 'PRON', 'VERB', 'PART', 'ADJ', 'PROPN', 'PUNCT', 'ADV', 'ADV']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# „Å™„Å´„Åã„Åß„Å¶„Åç„Åæ„Åó„Åü\n",
    "[rev_tag_dict[i] for i in np.argmax(Out[27], 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂçòË™û„É™„Çπ„Éà„ÇíÂÖ•Âäõ„Åó„Å¶POS„Çø„Ç∞„Çí‰∫àÊ∏¨„Åô„ÇãÈñ¢Êï∞\n",
    "def predict(words):\n",
    "    words = [PAD] * 3 + words + [PAD] * 3\n",
    "    ids = [word_dict.get(word, word_dict[UNK]) for word in words]\n",
    "    windows = sliding_windows(ids)\n",
    "    matrix = np.array(windows, 'i')\n",
    "    probabilities = model.predict(matrix)\n",
    "    result_ids = np.argmax(probabilities, 1)\n",
    "    result = [rev_tag_dict[i] for i in result_ids]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRON', 'VERB', 'DET', 'NOUN', 'NOUN', 'PUNCT']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(['this', 'is', 'a', 'test', 'sentence', '.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:\t the president , at a news conference friday , also renewed a call for the ouster of panama 's noriega .\n",
      "predict:\t DET NOUN PUNCT ADP DET NOUN NOUN PROPN PUNCT ADV VERB DET NOUN ADP DET NOUN ADP PROPN PART PROPN PUNCT\n",
      "answer:\t DET NOUN PUNCT ADP DET NOUN NOUN PROPN PUNCT ADV VERB DET NOUN ADP DET NOUN ADP PROPN PART PROPN PUNCT\n",
      "sentence:\t also , the company 's hair-growing drug , rogaine , is selling well -- at about $ 125 million for the year , but the company 's profit from the drug has been reduced by upjohn 's expensive print and television campaigns for advertising , analysts said .\n",
      "predict:\t ADV PUNCT DET NOUN PART ADJ NOUN PUNCT ADJ PUNCT AUX VERB ADV PUNCT ADP ADP SYM NUM NUM ADP DET NOUN PUNCT CONJ DET NOUN PART NOUN ADP DET NOUN AUX AUX VERB ADP PROPN PART ADJ NOUN CONJ NOUN NOUN ADP NOUN PUNCT NOUN VERB PUNCT\n",
      "answer:\t ADV PUNCT DET NOUN PART ADJ NOUN PUNCT PROPN PUNCT AUX VERB ADV PUNCT ADP ADV SYM NUM NUM ADP DET NOUN PUNCT CONJ DET NOUN PART NOUN ADP DET NOUN AUX AUX VERB ADP PROPN PART ADJ NOUN CONJ NOUN NOUN ADP VERB PUNCT NOUN VERB PUNCT\n",
      "sentence:\t the dollar also began to decline friday as the stock market 's plunge caused some investors to reassess their desire to invest in the u.s. .\n",
      "predict:\t DET NOUN ADV VERB PART VERB PROPN SCONJ DET NOUN NOUN PART NOUN VERB DET NOUN PART VERB PRON NOUN PART VERB ADP DET PROPN PUNCT\n",
      "answer:\t DET NOUN ADV VERB PART VERB PROPN SCONJ DET NOUN NOUN PART NOUN VERB DET NOUN PART VERB PRON NOUN PART VERB ADP DET PROPN PUNCT\n",
      "sentence:\t this market still wants to go higher , said nauman barakat , a first vice president at shearson lehman hutton inc .\n",
      "predict:\t DET NOUN ADV VERB PART VERB ADJ PUNCT VERB PROPN PROPN PUNCT DET ADJ NOUN NOUN ADP PROPN PROPN PROPN PROPN PUNCT\n",
      "answer:\t DET NOUN ADV VERB PART VERB ADV PUNCT VERB PROPN PROPN PUNCT DET ADJ NOUN NOUN ADP PROPN PROPN PROPN PROPN PUNCT\n",
      "sentence:\t i have this feeling that it 's built on sand , she says , that the market rises but there 's no foundation to it .\n",
      "predict:\t PRON AUX DET VERB SCONJ PRON AUX VERB ADP NOUN PUNCT PRON VERB PUNCT SCONJ DET NOUN VERB CONJ PRON VERB DET NOUN ADP PRON PUNCT\n",
      "answer:\t PRON VERB DET NOUN SCONJ PRON AUX VERB ADP NOUN PUNCT PRON VERB PUNCT SCONJ DET NOUN VERB CONJ PRON VERB DET NOUN ADP PRON PUNCT\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for _ in range(5):\n",
    "    i = random.randint(0, len(test_sents))\n",
    "    words, tags = test_sents[i]\n",
    "    words = words[3:-3]\n",
    "    print('sentence:\\t', ' '.join(words))\n",
    "    print('predict:\\t', ' '.join(predict(words)))\n",
    "    print('answer:\\t', ' '.join(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('models/weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
