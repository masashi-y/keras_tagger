{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‰ªäÂõû„ÅØ„ÄÅfeedforward neural network„Çí‰Ωø„Å£„Å¶„ÄÅÂõ∫ÂÆö„Ç¶„Ç£„É≥„Éâ„Ç¶ÂπÖ(Â∑¶Âè≥Ôºì)„ÅÆÊÉÖÂ†±„Çí‰Ωø„Å£„Å¶ÂçòË™û„ÅÆÂìÅË©û„Çø„Ç∞„Çí‰∫àÊ∏¨„Åô„Çã„É¢„Éá„É´„ÇíÂ≠¶Áøí„Åó„Å¶„Åø„Åæ„ÅôÔºé  \n",
    "„Åã„Å™„Çä„Ç∑„É≥„Éó„É´„Åß„Åô„Åå„ÄÅÈ´òÈÄü„Å´‰∫àÊ∏¨„Åß„Åç„Çã„Å®„ÅÑ„ÅÜÈ≠ÖÂäõ„Åã„Çâ„ÄÅ„Çà„ÇäÂº∑Âäõ„Å™„É¢„Éá„É´„ÅåÊ≤¢Â±±Â≠òÂú®„Åô„Çã‰ªä„Åß„ÇÇ„ÄÅNLP„ÅÆ„ÅÑ„Çç„Çì„Å™„ÉÑ„Éº„É´„Åß‰Ωø„Çè„Çå„Å¶„Åæ„ÅôÔºé  \n",
    "easyccg: https://github.com/mikelewis0/easyccg  \n",
    "syntaxnet: https://github.com/tensorflow/models/tree/master/research/syntaxnet  \n",
    "stanford parser: https://nlp.stanford.edu/software/lex-parser.shtml  \n",
    "(„Ç§„É°„Éº„Ç∏)„ÄÄ„ÄÄ\n",
    "<img src='images/ff.png'>\n",
    "(ÁîªÂÉè: https://xbt.net/blog/what-is-enigma/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS„Çø„Ç∞(Part-Of-Speech, ÂìÅË©û„Çø„Ç∞)„ÅÆ„É™„Çπ„Éà:  \n",
    "- ADJ: adjective\n",
    "- ADP: adposition\n",
    "- ADV: adverb\n",
    "- AUX: auxiliary\n",
    "- CCONJ: coordinating conjunction\n",
    "- DET: determiner\n",
    "- INTJ: interjection\n",
    "- NOUN: noun\n",
    "- NUM: numeral\n",
    "- PART: particle\n",
    "- PRON: pronoun\n",
    "- PROPN: proper noun\n",
    "- PUNCT: punctuation\n",
    "- SCONJ: subordinating conjunction\n",
    "- SYM: symbol\n",
    "- VERB: verb\n",
    "- X: other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# „Å§„Åã„ÅÜ„É©„Ç§„Éñ„É©„É™„ÅÆË™≠„ÅøËæº„Åø\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, Input, Reshape, Concatenate\n",
    "from keras.optimizers import SGD\n",
    "from collections import Counter\n",
    "\n",
    "# üëáÁÑ°Ë¶ñ„Åó„Å¶OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto(\n",
    "    gpu_options=tf.GPUOptions(\n",
    "        visible_device_list=\"0\", # specify GPU number\n",
    "        allow_growth=True\n",
    "    )\n",
    ")\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoNLL„Éï„Ç©„Éº„Éû„ÉÉ„Éà„ÇíË™≠„ÅøËæº„ÇÄÈñ¢Êï∞\n",
    "def read_conll(file):\n",
    "    res = []\n",
    "    words = []\n",
    "    tags = []\n",
    "    for line in open(file):\n",
    "        line = line.strip()\n",
    "         # Á©∫Ë°å„Å™„Çâ„Åù„Çå„Åæ„Åß„Å´„Å§„Åè„Å£„ÅüÊñá„ÇíÂá∫Âäõ\n",
    "        if len(line) == 0:\n",
    "            res.append((words, tags))\n",
    "            words = []\n",
    "            tags = []\n",
    "        # ÂçòË™û„Å®„Çø„Ç∞„ÇíÂèñ„ÇäÂá∫„Åô\n",
    "        else:\n",
    "            items = line.split('\\t')\n",
    "            words.append(items[1].lower()) # Â∞èÊñáÂ≠ó„Å´„Åó„Å¶„Åä„Åè\n",
    "            tags.append(items[3])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Â≠¶Áøí„Éá„Éº„Çø (training data)\n",
    "train_sents = read_conll('data/train.conll')\n",
    "# Ë©ï‰æ°Áî®„Éá„Éº„Çø (test data)\n",
    "test_sents = read_conll('data/test.conll')\n",
    "# ÈñãÁô∫„Éá„Éº„Çø (development data)\n",
    "dev_sents = read_conll('data/dev.conll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prefix(word):\n",
    "    word = word.rjust(2, ' ')\n",
    "    return word[:2]\n",
    "\n",
    "def get_suffix(word):\n",
    "    word = word.ljust(2, ' ')\n",
    "    return word[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do og\n",
      "is is\n",
      " a a \n"
     ]
    }
   ],
   "source": [
    "# „ÉÜ„Çπ„Éà\n",
    "word = 'dog'\n",
    "print(get_prefix(word), get_suffix(word))\n",
    "word = 'is'\n",
    "print(get_prefix(word), get_suffix(word))\n",
    "word = 'a'\n",
    "print(get_prefix(word), get_suffix(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_windows(lst):\n",
    "    res = []\n",
    "    for i in range(len(lst) - 6):\n",
    "        res.append(lst[i:i+7])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = 'PAD'\n",
    "\n",
    "train_sents = [([PAD] * 3 + words + [PAD] * 3, tags) for words, tags in train_sents]\n",
    "test_sents = [([PAD] * 3 + words + [PAD] * 3, tags) for words, tags in test_sents]\n",
    "dev_sents = [([PAD] * 3 + words + [PAD] * 3, tags) for words, tags in dev_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(tokens, max_count=2, UNK='UNK'):\n",
    "    counter = Counter(tokens)\n",
    "    token_list = [token for token, count in counter.most_common() if count >= max_count]\n",
    "    token_list.append(UNK)\n",
    "    token_dict = {t: i for i, t in enumerate(token_list)}\n",
    "    return token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = 'UNK'\n",
    "# ÂçòË™û„ÅÆÂüã„ÇÅËæº„Åø„Éô„ÇØ„Éà„É´„Åå„ÅÜ„Åæ„Åè„ÅÑ„Åè„Åü„ÇÅ„Å´„ÅØ„ÄÅ„Åù„ÅÆÂçòË™û„Åå„ÅÑ„Çç„ÅÑ„Çç„Å™ÊñáËÑà„ÅßÂá∫Áèæ„Åó„Å¶„Åª„Åó„ÅÑÔºé\n",
    "# Â≠¶Áøí„Éá„Éº„Çø„Å´„Å°„Çá„Å£„Å®(ÔºíÂõû„Çà„Çä‰∏ã)„Åó„ÅãÂá∫„Å™„ÅÑÂçòË™û„ÅØUNK„ÅßÁΩÆ„ÅçÊèõ„Åà„ÇãÔºé\n",
    "word_dict = make_vocab(word for words, _ in train_sents for word in words)\n",
    "prefix_dict = make_vocab(get_prefix(word) for words, _ in train_sents for word in words)\n",
    "suffix_dict = make_vocab(get_suffix(word) for words, _ in train_sents for word in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS„Çø„Ç∞„ÇíËá™ÁÑ∂Êï∞„ÅÆID„Å´Â§âÊèõ„Åô„ÇãËæûÊõ∏\n",
    "tag_set = set(tag for _, tags in train_sents for tag in tags)\n",
    "tag_dict = {w: i for i, w in enumerate(tag_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_dict size 21569\n",
      "prefix_dict size 671\n",
      "suffix_dict size 662\n",
      "tag_dict size 17\n"
     ]
    }
   ],
   "source": [
    "print('word_dict size', len(word_dict))\n",
    "print('prefix_dict size', len(prefix_dict))\n",
    "print('suffix_dict size', len(suffix_dict))\n",
    "print('tag_dict size', len(tag_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_matrices(words_and_tags):\n",
    "    word_xs = []\n",
    "    prefix_xs = []\n",
    "    suffix_xs = []\n",
    "    ys = []\n",
    "    for words, tags in words_and_tags:\n",
    "        for window in sliding_windows(words):\n",
    "            word_xs.append([word_dict.get(word, word_dict[UNK]) for word in window])\n",
    "            prefix_xs.append([prefix_dict.get(get_prefix(word), prefix_dict[UNK]) for word in window])\n",
    "            suffix_xs.append([suffix_dict.get(get_suffix(word), suffix_dict[UNK]) for word in window])\n",
    "        ys.extend(tag_dict[tag] for tag in tags)\n",
    "\n",
    "    word_xs = np.array(word_xs, 'i')\n",
    "    prefix_xs = np.array(prefix_xs, 'i')\n",
    "    suffix_xs = np.array(suffix_xs, 'i')\n",
    "    ys = np.array(ys, 'i')\n",
    "    ys = keras.utils.to_categorical(ys, len(tag_dict))\n",
    "    print('dimensions of xs', word_xs.shape, prefix_xs.shape, suffix_xs.shape)\n",
    "    print('dimensions of ys', ys.shape)\n",
    "    return [word_xs, prefix_xs, suffix_xs], ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions of xs (929552, 7) (929552, 7) (929552, 7)\n",
      "dimensions of ys (929552, 17)\n",
      "dimensions of xs (55371, 7) (55371, 7) (55371, 7)\n",
      "dimensions of ys (55371, 17)\n",
      "dimensions of xs (45422, 7) (45422, 7) (45422, 7)\n",
      "dimensions of ys (45422, 17)\n"
     ]
    }
   ],
   "source": [
    "train_xs, train_ys = make_matrices(train_sents)\n",
    "test_xs, test_ys = make_matrices(test_sents)\n",
    "dev_xs, dev_ys = make_matrices(dev_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(word_input, prefix_input, suffix_input, word_vocab_size, prefix_vocab_size, suffix_vocab_size, n_tags,\n",
    "              WORD_EMBED_DIM=128, PREFIX_EMBED_DIM=32, SUFFIX_EMBED_DIM=32, HIDDEN1_DIM=256, HIDDEN2_DIM=128):\n",
    "    word_embed = Embedding(word_vocab_size, WORD_EMBED_DIM)(word_input)\n",
    "    prefix_embed = Embedding(prefix_vocab_size, PREFIX_EMBED_DIM)(prefix_input)\n",
    "    suffix_embed = Embedding(suffix_vocab_size, SUFFIX_EMBED_DIM)(suffix_input)\n",
    "    embed = Concatenate()([word_embed, prefix_embed, suffix_embed])\n",
    "    embed_reshaped = Reshape((-1,))(embed)\n",
    "    hidden1 = Dense(HIDDEN1_DIM, activation='tanh')(embed_reshaped)\n",
    "    hidden2 = Dense(HIDDEN2_DIM, activation='tanh')(hidden1)\n",
    "    logits = Dense(n_tags, activation='softmax')(hidden2)\n",
    "    model = keras.Model(inputs=[word_input, prefix_input, suffix_input], outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_input, prefix_input, suffix_input = Input(shape=(7,)), Input(shape=(7,)), Input(shape=(7,))\n",
    "model = make_model(word_input, prefix_input, suffix_input, len(word_dict), len(prefix_dict), len(suffix_dict), len(tag_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-597b8cd04e4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'images/model.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rankdir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         raise ImportError(\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;34m'Failed to import `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;34m'Please install `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(model, 'images/model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÂçòË™û„ÅÆIDÂàó${\\bf x} = x_{-2}, x_{-1}, x, x_{+1}, x_{+2}$„Å´ÂØæ„Åó„Å¶  \n",
    "$Embedding(\\bf x) = [ {\\bf e}_{x_{-2}}„ÄÄ| {\\bf e}_{x_{-1}}„ÄÄ| {\\bf e}_{x}„ÄÄ| {\\bf e}_{x_{+1}}„ÄÄ|„ÄÄ{\\bf e}_{x_{+2}} ]^T„ÄÄ= {\\bf E}^T$,  \n",
    "$ Reshape({\\bf E}) = [ {\\bf e}_{x_{-2}}, {\\bf e}_{x_{-1}}, {\\bf e}_{x}, {\\bf e}_{x_{+1}}, {\\bf e}_{x_{+2}} ]^T = {\\bf e}$ (Á∏¶„Å´‰∏¶„Åπ„Çã),  \n",
    "$f({\\bf x}) = {\\mathit softmax}(W_3 \\tanh (W_2 \\tanh (W_1 {\\bf e} + b_1) + b_2) + b_3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ë®àÁÆó„Ç∞„É©„Éï„ÅÆÂèØË¶ñÂåñ\n",
    "<img src='images/model.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 7)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 7)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 7)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 7, 128)       2760832     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 7, 32)        21472       input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 7, 32)        21184       input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 7, 192)       0           embedding_7[0][0]                \n",
      "                                                                 embedding_8[0][0]                \n",
      "                                                                 embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1344)         0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 256)          344320      reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 128)          32896       dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 17)           2193        dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,182,897\n",
      "Trainable params: 3,182,897\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 4787645517783878182, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 7082170778\n",
       " locality {\n",
       "   bus_id: 1\n",
       " }\n",
       " incarnation: 18102384936197633674\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2\", name: \"/device:GPU:1\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 6375404340\n",
       " locality {\n",
       "   bus_id: 1\n",
       " }\n",
       " incarnation: 9913575533825859606\n",
       " physical_device_desc: \"device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0, compute capability: 5.2\"]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 929552 samples, validate on 45422 samples\n",
      "Epoch 1/200\n",
      "929552/929552 [==============================] - 41s 44us/step - loss: 2.4042 - acc: 0.2473 - val_loss: 2.1184 - val_acc: 0.3817\n",
      "Epoch 2/200\n",
      "111616/929552 [==>...........................] - ETA: 36s - loss: 2.0891 - acc: 0.3932"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-d9f16eb5c4ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Â≠¶Áøí\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_ys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Â≠¶Áøí„Åå„ÇÅ„Çì„Å©„ÅÑÂ†¥Âêà„Åì„Å£„Å° (Â≠¶ÁøíÊ∏à„Åø„ÅÆ„Éë„É©„É°„Éº„Çø„ÇíË™≠„ÅøËæº„Åø)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# model.load_weights('models/weights.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Â≠¶Áøí\n",
    "model.fit(train_xs, train_ys, batch_size=1024, epochs=200, verbose=1, validation_data=(dev_xs, dev_ys))\n",
    "\n",
    "# Â≠¶Áøí„Åå„ÇÅ„Çì„Å©„ÅÑÂ†¥Âêà„Åì„Å£„Å° (Â≠¶ÁøíÊ∏à„Åø„ÅÆ„Éë„É©„É°„Éº„Çø„ÇíË™≠„ÅøËæº„Åø)\n",
    "# model.load_weights('models/weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.10557082e-02, 1.32073769e-02, 7.10472639e-04, 7.18352795e-01,\n",
       "        5.89033652e-06, 3.89746914e-04, 5.57578518e-04, 2.58904400e-08,\n",
       "        1.58037595e-03, 2.01247330e-03, 1.02006830e-03, 7.10095046e-04,\n",
       "        4.66211577e-06, 2.16930785e-04, 3.60750452e-09, 1.80175677e-01,\n",
       "        7.04472061e-16],\n",
       "       [7.86494547e-06, 2.18097651e-09, 1.24326913e-08, 6.10316864e-13,\n",
       "        5.12558381e-06, 3.25297265e-07, 2.90912703e-05, 5.02471835e-07,\n",
       "        8.15867799e-16, 5.47013144e-07, 9.99931097e-01, 5.55866075e-10,\n",
       "        3.77816776e-14, 2.23222092e-12, 2.88937202e-14, 2.54073948e-05,\n",
       "        2.39162648e-17],\n",
       "       [5.88764760e-06, 9.99991417e-01, 3.21845732e-08, 2.37876435e-07,\n",
       "        4.31857744e-11, 3.83840426e-09, 3.75526753e-07, 3.62298209e-07,\n",
       "        8.90485342e-12, 1.44225919e-06, 1.40812717e-09, 4.49098270e-09,\n",
       "        4.32071739e-12, 1.78185413e-08, 1.58697949e-16, 3.02612193e-07,\n",
       "        8.71168284e-21],\n",
       "       [2.78762332e-06, 2.99380929e-07, 6.22993355e-08, 1.89591069e-11,\n",
       "        1.18249865e-07, 1.12220647e-07, 6.04438222e-10, 1.85235992e-01,\n",
       "        5.48263171e-11, 1.28865878e-08, 2.74877276e-09, 4.82539910e-08,\n",
       "        6.48731069e-10, 3.09250083e-07, 8.14760208e-01, 4.03657874e-13,\n",
       "        1.05973879e-08],\n",
       "       [1.55373523e-03, 1.84848297e-08, 1.27983483e-07, 3.61202375e-08,\n",
       "        1.99484120e-07, 9.98344898e-01, 1.66643431e-07, 1.11149518e-06,\n",
       "        5.93643961e-08, 9.60529214e-05, 1.16685487e-06, 1.38816958e-07,\n",
       "        2.12380428e-06, 1.55193938e-08, 2.84002266e-09, 4.37437361e-17,\n",
       "        7.41189291e-21],\n",
       "       [3.38509599e-05, 3.43298345e-09, 8.48578452e-07, 5.98736115e-05,\n",
       "        1.94610238e-05, 2.16799965e-08, 4.65684842e-08, 3.34189981e-10,\n",
       "        9.97890413e-01, 1.27522048e-09, 1.06249828e-07, 3.97084732e-05,\n",
       "        6.85932446e-06, 4.27011810e-06, 1.46860583e-03, 1.02734513e-04,\n",
       "        3.73150455e-04],\n",
       "       [2.15060467e-07, 1.30863654e-07, 8.88689371e-08, 1.33376261e-07,\n",
       "        1.53163683e-06, 1.44930859e-10, 2.55603947e-08, 2.93949262e-12,\n",
       "        6.53779466e-07, 3.55013935e-11, 5.47567026e-07, 2.31251832e-07,\n",
       "        2.78216700e-10, 5.89388804e-08, 3.30027700e-10, 9.99996066e-01,\n",
       "        3.37383227e-07],\n",
       "       [5.95342044e-06, 1.21922944e-10, 6.79182888e-09, 3.66117808e-13,\n",
       "        9.38162793e-06, 1.22710344e-06, 4.08636424e-06, 2.96525915e-08,\n",
       "        5.66208618e-15, 4.20304396e-08, 9.99974370e-01, 5.50276935e-10,\n",
       "        8.51077347e-13, 9.67689687e-13, 4.12879718e-13, 4.90602224e-06,\n",
       "        7.24227358e-16],\n",
       "       [9.81510162e-01, 8.19611669e-05, 1.43830066e-05, 8.13563020e-07,\n",
       "        2.96430926e-07, 2.92374374e-04, 9.35229997e-04, 3.83749330e-06,\n",
       "        8.13145675e-07, 1.70841292e-02, 2.90051648e-05, 5.34786341e-06,\n",
       "        4.16305156e-05, 1.45383567e-07, 2.79617052e-10, 7.36621208e-10,\n",
       "        3.66972048e-16],\n",
       "       [9.98727620e-01, 7.58225360e-05, 1.28597254e-04, 6.25387713e-07,\n",
       "        4.86604004e-05, 3.42586668e-06, 1.14824201e-04, 1.25931717e-06,\n",
       "        5.70837059e-04, 8.22627226e-06, 3.24577850e-05, 1.64633122e-04,\n",
       "        2.74407762e-06, 4.46088325e-06, 4.67201280e-05, 6.78745782e-05,\n",
       "        1.22796371e-06]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Â≠¶Áøí„Åó„Åü„É¢„Éá„É´„ÇíÈÅ©ÂΩì„Å´‰Ωø„Å£„Å¶„Åø„Çã„Å®„Å™„Çì„ÅãË°åÂàó„ÅåÂá∫„Å¶„Åç„Åæ„Åô\n",
    "model.predict(test_xs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 17)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# „Åù„ÅÆË°åÂàó„ÅÆÂΩ¢\n",
    "Out[27].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS„Çø„Ç∞„Å®ID„ÅÆÈÄÜÂêë„Åç„ÅÆËæûÊõ∏\n",
    "rev_tag_dict = {v: k for k, v in tag_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DET', 'PUNCT', 'PRON', 'VERB', 'PART', 'ADJ', 'PROPN', 'PUNCT', 'ADV', 'ADV']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# „Å™„Å´„Åã„Åß„Å¶„Åç„Åæ„Åó„Åü\n",
    "[rev_tag_dict[i] for i in np.argmax(Out[27], 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂçòË™û„É™„Çπ„Éà„ÇíÂÖ•Âäõ„Åó„Å¶POS„Çø„Ç∞„Çí‰∫àÊ∏¨„Åô„ÇãÈñ¢Êï∞\n",
    "def predict(words):\n",
    "    words = [PAD] * 3 + words + [PAD] * 3\n",
    "    ids = [word_dict.get(word, word_dict[UNK]) for word in words]\n",
    "    windows = sliding_windows(ids)\n",
    "    matrix = np.array(windows, 'i')\n",
    "    probabilities = model.predict(matrix)\n",
    "    result_ids = np.argmax(probabilities, 1)\n",
    "    result = [rev_tag_dict[i] for i in result_ids]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRON', 'VERB', 'DET', 'NOUN', 'NOUN', 'PUNCT']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(['this', 'is', 'a', 'test', 'sentence', '.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:\t the president , at a news conference friday , also renewed a call for the ouster of panama 's noriega .\n",
      "predict:\t DET NOUN PUNCT ADP DET NOUN NOUN PROPN PUNCT ADV VERB DET NOUN ADP DET NOUN ADP PROPN PART PROPN PUNCT\n",
      "answer:\t DET NOUN PUNCT ADP DET NOUN NOUN PROPN PUNCT ADV VERB DET NOUN ADP DET NOUN ADP PROPN PART PROPN PUNCT\n",
      "sentence:\t also , the company 's hair-growing drug , rogaine , is selling well -- at about $ 125 million for the year , but the company 's profit from the drug has been reduced by upjohn 's expensive print and television campaigns for advertising , analysts said .\n",
      "predict:\t ADV PUNCT DET NOUN PART ADJ NOUN PUNCT ADJ PUNCT AUX VERB ADV PUNCT ADP ADP SYM NUM NUM ADP DET NOUN PUNCT CONJ DET NOUN PART NOUN ADP DET NOUN AUX AUX VERB ADP PROPN PART ADJ NOUN CONJ NOUN NOUN ADP NOUN PUNCT NOUN VERB PUNCT\n",
      "answer:\t ADV PUNCT DET NOUN PART ADJ NOUN PUNCT PROPN PUNCT AUX VERB ADV PUNCT ADP ADV SYM NUM NUM ADP DET NOUN PUNCT CONJ DET NOUN PART NOUN ADP DET NOUN AUX AUX VERB ADP PROPN PART ADJ NOUN CONJ NOUN NOUN ADP VERB PUNCT NOUN VERB PUNCT\n",
      "sentence:\t the dollar also began to decline friday as the stock market 's plunge caused some investors to reassess their desire to invest in the u.s. .\n",
      "predict:\t DET NOUN ADV VERB PART VERB PROPN SCONJ DET NOUN NOUN PART NOUN VERB DET NOUN PART VERB PRON NOUN PART VERB ADP DET PROPN PUNCT\n",
      "answer:\t DET NOUN ADV VERB PART VERB PROPN SCONJ DET NOUN NOUN PART NOUN VERB DET NOUN PART VERB PRON NOUN PART VERB ADP DET PROPN PUNCT\n",
      "sentence:\t this market still wants to go higher , said nauman barakat , a first vice president at shearson lehman hutton inc .\n",
      "predict:\t DET NOUN ADV VERB PART VERB ADJ PUNCT VERB PROPN PROPN PUNCT DET ADJ NOUN NOUN ADP PROPN PROPN PROPN PROPN PUNCT\n",
      "answer:\t DET NOUN ADV VERB PART VERB ADV PUNCT VERB PROPN PROPN PUNCT DET ADJ NOUN NOUN ADP PROPN PROPN PROPN PROPN PUNCT\n",
      "sentence:\t i have this feeling that it 's built on sand , she says , that the market rises but there 's no foundation to it .\n",
      "predict:\t PRON AUX DET VERB SCONJ PRON AUX VERB ADP NOUN PUNCT PRON VERB PUNCT SCONJ DET NOUN VERB CONJ PRON VERB DET NOUN ADP PRON PUNCT\n",
      "answer:\t PRON VERB DET NOUN SCONJ PRON AUX VERB ADP NOUN PUNCT PRON VERB PUNCT SCONJ DET NOUN VERB CONJ PRON VERB DET NOUN ADP PRON PUNCT\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for _ in range(5):\n",
    "    i = random.randint(0, len(test_sents))\n",
    "    words, tags = test_sents[i]\n",
    "    words = words[3:-3]\n",
    "    print('sentence:\\t', ' '.join(words))\n",
    "    print('predict:\\t', ' '.join(predict(words)))\n",
    "    print('answer:\\t', ' '.join(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('models/weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
